{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "AC Implementation.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "73-xtOIiGQBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QxF_LW6GQBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.envs.make('CartPole-v1')\n",
        "action_space = list(range(env.action_space.n))\n",
        "policy_lr = 0.01 # learning rate for the policy net\n",
        "policy_lr_decay = 0.001 # decay factor for the policy net learning rate"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAmzaR3YGQBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy_network = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dense(env.action_space.n, activation='softmax')\n",
        "])\n",
        "policy_network.build([None, 4])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmNPyH1hGQB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "theta = policy_network.trainable_variables\n",
        "gamma = 0.9"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PunbexKMGQCL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy_optimizer = tf.keras.optimizers.SGD(lr=policy_lr, decay=policy_lr_decay)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGt6bKH6GQCY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_episode_PG(env, policy_net, train=True, render=True):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    t = 0\n",
        "    episode = []\n",
        "    while not done:\n",
        "        # unsqueeze state vector to [1 x state size]\n",
        "        state_ = state[None, ...]\n",
        "\n",
        "        # get the policy distribution for state\n",
        "        action_distribution = policy_net(state_)\n",
        "\n",
        "        # sample an action as per the policy distribution\n",
        "        action = np.random.choice(action_space, p=action_distribution.numpy()[0])\n",
        "        \n",
        "        # act on the environment\n",
        "        new_state, reward, done, __md = env.step(action)\n",
        "        \n",
        "        # append the experience tuple to the episode array\n",
        "        episode.append((t, state, action, reward))\n",
        "        if render:\n",
        "            env.render()\n",
        "        state = new_state\n",
        "        t += 1\n",
        "    return episode"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUpzDNuaGQCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_EPISODES = 2000\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRshdKxJGQCu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "outputId": "ea52c38b-adfc-4426-ab90-0b5e1d9916c1"
      },
      "source": [
        "rewards, average_rewards = [0], [0]\n",
        "for episode_number in range(NUM_EPISODES):\n",
        "\n",
        "    episode = generate_episode_PG(env, policy_network, render=False)\n",
        "\n",
        "    G = 0\n",
        "\n",
        "    # iterate over experience tuples\n",
        "    for i, state, action, reward in reversed(episode):\n",
        "        with tf.GradientTape() as tape:\n",
        "            G = G * gamma + reward\n",
        "\n",
        "            # calculate pi(s, a)\n",
        "            action_probability = policy_network(state[None, ...])[0, action]\n",
        "\n",
        "            # calculate log(pi(s,a))\n",
        "            log_action_probability = tf.math.log(action_probability)\n",
        "\n",
        "        # calculate grad(log(pi(s,a))) wrt theta\n",
        "        grads = tape.gradient(log_action_probability, theta)\n",
        "\n",
        "        # calculate PG -- the minus sign makes the optimizer ascend the PG\n",
        "        policy_gradients = [- G * g for g in grads]\n",
        "\n",
        "        # update the parameters\n",
        "        policy_optimizer.apply_gradients(zip(policy_gradients, theta))\n",
        "\n",
        "\n",
        "    # record the total rewards for this timestep\n",
        "    # this produces a noisy plot\n",
        "    rewards.append(len(episode))\n",
        "\n",
        "    # record a rolling average of total rewards across episodes\n",
        "    average_rewards.append(average_rewards[-1]*0.9 + len(episode)*0.1)\n",
        "\n",
        "    # print stats every 50 episodes\n",
        "    if not (episode_number+1) % 50:\n",
        "        print(f'Episode: {episode_number+1}, average lifetime:{round(average_rewards[-1])}.')\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Episode: 50, average lifetime:10.\n",
            "Episode: 100, average lifetime:10.\n",
            "Episode: 150, average lifetime:24.\n",
            "Episode: 200, average lifetime:21.\n",
            "Episode: 250, average lifetime:18.\n",
            "Episode: 300, average lifetime:27.\n",
            "Episode: 350, average lifetime:53.\n",
            "Episode: 400, average lifetime:36.\n",
            "Episode: 450, average lifetime:84.\n",
            "Episode: 500, average lifetime:50.\n",
            "Episode: 550, average lifetime:36.\n",
            "Episode: 600, average lifetime:61.\n",
            "Episode: 650, average lifetime:165.\n",
            "Episode: 700, average lifetime:125.\n",
            "Episode: 750, average lifetime:250.\n",
            "Episode: 800, average lifetime:198.\n",
            "Episode: 850, average lifetime:120.\n",
            "Episode: 900, average lifetime:69.\n",
            "Episode: 950, average lifetime:78.\n",
            "Episode: 1000, average lifetime:176.\n",
            "Episode: 1050, average lifetime:281.\n",
            "Episode: 1100, average lifetime:173.\n",
            "Episode: 1150, average lifetime:111.\n",
            "Episode: 1200, average lifetime:121.\n",
            "Episode: 1250, average lifetime:218.\n",
            "Episode: 1300, average lifetime:157.\n",
            "Episode: 1350, average lifetime:132.\n",
            "Episode: 1400, average lifetime:111.\n",
            "Episode: 1450, average lifetime:82.\n",
            "Episode: 1500, average lifetime:59.\n",
            "Episode: 1550, average lifetime:83.\n",
            "Episode: 1600, average lifetime:80.\n",
            "Episode: 1650, average lifetime:65.\n",
            "Episode: 1700, average lifetime:81.\n",
            "Episode: 1750, average lifetime:96.\n",
            "Episode: 1800, average lifetime:94.\n",
            "Episode: 1850, average lifetime:94.\n",
            "Episode: 1900, average lifetime:124.\n",
            "Episode: 1950, average lifetime:134.\n",
            "Episode: 2000, average lifetime:137.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRnLUzNTGQC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u2GQIl1GQDI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RngXuB-WGQDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Es6_uMDGQDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vwPFWABGQDs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MX2DX_LgGQD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}